# AllYouNeedIsAttention

This GitHub repository provides a basic implementation of the "Attention Is All You Need" paper (https://arxiv.org/abs/1706.03762), with the primary objective of gaining a better understanding of the attention mechanism in language models.

## Table of Contents

1. [Introduction](#introduction)
2. [Requirements and Installation](#requirements-and-installation)
3. [Usage](#usage)
4. [Contributing](#contributing)
5. [License](#license)
6. [Citations](#citations)

## Introduction

The "Attention Is All You Need" paper introduced the Transformer model, a novel architecture that leverages self-attention mechanisms to achieve state-of-the-art results in various NLP tasks. 

![Attention is All You Need Architecture](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)


[**_Source_**](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)

## Requirements and Installation

### Requirements

- Python 3.7 or higher
- PyTorch 1.8.0 or higher
- Hugging Face Transformers library
- NumPy
- tqdm

### Installation

Clone the repository and install the required packages:

```bash
git clone https://github.com/p1kit/AllYouNeedIsAttention.git
cd AllYouNeedIsAttention
pip install -r requirements.txt
```
